{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as sp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.test_validation_split import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = os.path.dirname(os.path.realpath(__name__))\n",
    "path = f\"{p}/data/data.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SUBJECTS = 10\n",
    "N_CONDITIONS = 3\n",
    "N_REPS = 10\n",
    "N_SENSORS = 6\n",
    "N_TRIALS = N_SUBJECTS * N_CONDITIONS * N_REPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = [\"subject\", \"condition\", \"replication\"]\n",
    "index_columns = trials + [\"time\"]\n",
    "df = df.set_index(index_columns)\n",
    "\n",
    "matrix = pd.DataFrame()\n",
    "joint_map = {1: \"ankle\", 2: \"knee\", 3: \"hip\"}\n",
    "leg_map = {1: \"left\", 2: \"right\"}\n",
    "\n",
    "for leg_key, leg_val in leg_map.items():\n",
    "    for joint_key, joint_val in joint_map.items():\n",
    "        matrix[f\"{leg_val}_{joint_val}\"] = df[(df.leg == leg_key) & (df.joint == joint_key)].angle\n",
    "\n",
    "rs = matrix.reset_index()\n",
    "series = pd.DataFrame((rs.subject - 1) * N_REPS * N_CONDITIONS + (rs.condition - 1) * N_REPS + rs.replication)\n",
    "matrix[\"trial\"] = series.set_index(matrix.index)\n",
    "\n",
    "target = pd.Series(range(N_TRIALS), index=range(1, N_TRIALS + 1))\n",
    "target = 1 + ((target // 10) % 3)\n",
    "target.name = \"condition\"\n",
    "\n",
    "matrix = matrix.reset_index().drop(trials, axis=1).set_index([\"trial\", \"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = matrix.groupby(\"trial\").mean()\n",
    "means.columns=[f\"{col}_mean\" for col in matrix.columns]\n",
    "stdevs = matrix.groupby(\"trial\").std()\n",
    "stdevs.columns=[f\"{col}_stdev\" for col in matrix.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = pd.DataFrame()\n",
    "covs = pd.DataFrame()\n",
    "\n",
    "for body_part in [\"ankle\", \"knee\"]:\n",
    "\n",
    "    first_half = matrix[matrix.index.get_level_values(1) < 50]\n",
    "    second_half = matrix[matrix.index.get_level_values(1) > 50]\n",
    "\n",
    "    body_parts = second_half[[f\"left_{body_part}\", f\"right_{body_part}\"]]\n",
    "    corrs[body_part] = body_parts.groupby([\"trial\"]).corr(method=\"pearson\").groupby(\"trial\")[f\"right_{body_part}\"].first()\n",
    "    covs[body_part] = body_parts.groupby([\"trial\"]).cov().groupby(\"trial\")[f\"right_{body_part}\"].first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([covs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.loc[41, :].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two sets of features:\n",
    "# base: mean of some of the sensors\n",
    "# correlation model\n",
    "# fft model\n",
    "\n",
    "# extension: only look at the second half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft, ifft, fftfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_splits(train_validation_data, targets, number_of_folds):\n",
    "    \n",
    "    # Convert input data to numpy array if it's not already\n",
    "    train_validation_index = train_validation_data.index\n",
    "    train_validation_data = np.array(train_validation_data)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    # Check if input_data and targets have the same number of rows\n",
    "    if train_validation_data.shape[0] != targets.shape[0]:\n",
    "        print(\"Input Data and Targets do not have the same number of entries.\")\n",
    "        print(f\"input_data.shape = {train_validation_data.shape}\")\n",
    "\n",
    "    # Randomly assign each data point to a fold\n",
    "    fold_assignments = np.random.randint(0, number_of_folds, size=targets.size)\n",
    "    fold_assignments = (train_validation_index.values - 1) // 30\n",
    "    print(fold_assignments)\n",
    "\n",
    "    # Saving the different splits in a list\n",
    "    folds = []\n",
    "\n",
    "    for f in range(number_of_folds):\n",
    "        train_filter = (fold_assignments != f)\n",
    "        valid_filter = ~train_filter\n",
    "\n",
    "        train_inputs = train_validation_data[train_filter, :]\n",
    "        train_targets = targets[train_filter]\n",
    "        valid_inputs = train_validation_data[valid_filter, :]\n",
    "        valid_targets = targets[valid_filter]\n",
    "\n",
    "        fold = {\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_targets\": train_targets,\n",
    "            \"valid_inputs\": valid_inputs,\n",
    "            \"valid_targets\": valid_targets\n",
    "        }\n",
    "        folds.append(fold)\n",
    "\n",
    "        print(f\"For fold {f}\")\n",
    "        print(f\"\\ttrain_inputs.shape = {train_inputs.shape}\")\n",
    "        print(f\"\\ttrain_targets.shape = {train_targets.shape}\")\n",
    "        print(f\"\\tvalid_inputs.shape = {valid_inputs.shape}\")\n",
    "        print(f\"\\tvalid_targets.shape = {valid_targets.shape}\")\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(\n",
    "    X_train,\n",
    "    y_train, \n",
    "    folds, \n",
    "    model,\n",
    "    metric\n",
    "    ):\n",
    "\n",
    "    # logic of CV\n",
    "    n = len(X_train)\n",
    "    n_per_fold = n // folds\n",
    "    metric_values = np.zeros(folds)\n",
    "\n",
    "    for i in range(folds):\n",
    "        validation_index = range(i * n_per_fold, (i+1) * n_per_fold)\n",
    "        X_train_fold = X_train[~X_train.index.isin(validation_index)]\n",
    "        y_train_fold = y_train[~y_train.index.isin(validation_index)]\n",
    "        X_test_fold = X_train[X_train.index.isin(validation_index)]\n",
    "        y_test_fold = y_train[y_train.index.isin(validation_index)]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "        metric_values[i] = (metric(y_test_fold, y_pred_fold))\n",
    "\n",
    "    return metric_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, ensemble, discriminant_analysis\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, log_loss, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functions.cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SUBJECTS = 7\n",
    "TRAIN_TRIALS = N_TRIALS * TRAIN_SUBJECTS / N_SUBJECTS\n",
    "\n",
    "features_used = covs\n",
    "target_used = target\n",
    "\n",
    "X = features_used.copy()\n",
    "y = target_used.copy()\n",
    "\n",
    "X_split = TRAIN_TRIALS\n",
    "y_split = (X_split)\n",
    "\n",
    "X_train = X.loc[:X_split]\n",
    "y_train = y.loc[:y_split]\n",
    "X_test = X.loc[X_split+1:]\n",
    "y_test = y.loc[y_split +1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a fn that takes features and targets, then spits out the results\n",
    "# within this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cross_validation_splits(X_train, y_train, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will use cross validation on X_train, y_train (leaving out 1 subject each time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can change this, need to have reasoning for the model\n",
    "model = linear_model.LogisticRegression(max_iter=1000, multi_class=\"multinomial\") \n",
    "model = ensemble.RandomForestClassifier()\n",
    "# model = discriminant_analysis.LinearDiscriminantAnalysis(solver=\"svd\")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(features.corr(), center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "for f in [confusion_matrix, accuracy_score]:\n",
    "    print(str(f).split()[1])\n",
    "    print(f(y_test, y_pred))\n",
    "\n",
    "# wants clear divisions between classes - would work\n",
    "print(log_loss(y_test, y_probs))\n",
    "\n",
    "# good scores - so look at the errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_probs).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([pd.Series(y_test).reset_index(drop=True), pd.Series(y_pred)], axis=1, ignore_index=False).sort_values(by=\"condition\")\n",
    "results.columns =[\"true\", \"pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ten subjects\n",
    "# ten replications\n",
    "# three conditions\n",
    "\n",
    "# 6 measurements for each one (two legs, three joints)\n",
    "# time series are 101 points long\n",
    "# so the data is actually (300 x 101 x 6), so we should represent it as such.\n",
    "\n",
    "# train-test split: \n",
    "# 7 subjects for train: should do leave-one-out validation (as in 1 subject each time)\n",
    "# 3 subjects for test: leave three subjects for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
